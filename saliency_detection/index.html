<!DOCTYPE html>
<html>
<title>Saliency Detection</title>

<head>
  <meta charset="UTF-8" />
  <link rel="shortcut icon" href="../favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="https://unpkg.com/element-ui/lib/theme-chalk/index.css" />
  <link rel="stylesheet" href="../css/index.css" />
  <script src="https://unpkg.com/vue/dist/vue.js"></script>
  <script src="https://unpkg.com/element-ui/lib/index.js"></script>
  <script src="https://unpkg.com/axios/dist/axios.min.js"></script>
</head>

<body>
  <div id="app">
    <el-page-header @back="goBack" title="Back"> </el-page-header>
    <!-- Saliency Detection -->
    <div class="content">
      <h1>Saliency Detection</h1>
      <el-divider></el-divider>
      <p>
        The human visual system can quickly identify regions in a scene that
        attract our attention. In this project, we are developing techniques
        to automatically detect salient objects from the input images and
        video.
      </p>
      <el-divider></el-divider>
      <div v-for="project,index in projects">
        <p class="project_p">
          <span v-html="textBold(project.authors)"></span>
          <b class="project_title">{{project.title}}</b>
          <span v-for="tag in project.tags" :style="styles[tag[1]]">{{tag[0]}}</span>
          <span v-for="link in project.links">[<b><a class="project_links"
                :href="link.link">{{link.name}}</a></b><span>]&nbsp;</span></span>
        </p>
        <div v-for="img in project.imgs" class="figure_div">
          <img :src="'figures/'+img.src" :alt="img" class="figure_img" />
          <div v-if="img.label">
            <div class="figure_label">{{img.label}}</div>
            <br />
          </div>
        </div>
        <div v-if="project.io">
          <p><b>Input-Output:&nbsp;</b>{{project.io}}<span></span></p>
        </div>
        <div v-if="project.abstract">
          <p><b>Abstract.&nbsp;</b>{{project.abstract}}<span></span></p>
        </div>
        <el-divider content-position="right">{{index+1}}</el-divider>
      </div>
    </div>
  </div>
</body>
<script src="../js/index.js"></script>
<script>
  new Vue({
    el: "#app",
    data: function () {
      return {
        styles,
        projects: [
          {
            authors: "Xin Tian, Ke Xu, Xin Yang, Baocai Yin, Rynson Lau.",
            title: "Weakly-supervised Salient Instance Detection.",
            tags: [
              ["BMVC 2020", 1],
              [". ", 0],
              ["(Oral, 5%, Best Student Paper)", 2],
            ],
            abstract:
              "Existing salient instance detection (SID) methods typically learn from pixel-level annotated datasets. In this paper, we present the first weakly-supervised approach to the SID problem. Although weak supervision has been considered in general saliency detection, it is mainly based on using class labels for object localization. However, it is non-trivial to use only class labels to learn instance-aware saliency information, as salient instances with high semantic affinities may not be easily separated by the labels. We note that subitizing information provides an instant judgement on the number of salient items, which naturally relates to detecting salient instances and may help separate instances of the same class while grouping different parts of the same instance. Inspired by this insight, we propose to use class and subitizing labels as weak supervision for the SID problem. We propose a novel weakly-supervised network with three branches: a Saliency Detection Branch leveraging class consistency information to locate candidate objects; a Boundary Detection Branch exploiting class discrepancy information to delineate object boundaries; and a Centroid Detection Branch using subitizing information to detect salient instance centroids. This complementary information is further fused to produce salient instance maps. We conduct extensive experiments to demonstrate that the proposed method plays favorably against carefully designed baseline methods adapted from related tasks. ",
            imgs: [
              { src: "Weakly-supervised Salient Instance Detection.png" },
            ],
          },
          {
            authors:
              "Shimin Zhao, Miaomiao Chen, Pengjie Wang, Ying Cao, Pingping Zhang and Xin Yang.",
            title:
              "RGB-D Salient Object Detection via Deep Fusion of Semantics and Details.",
            tags: [
              ["Journal of Computer Animation and Virtual Worlds", 1],
              [", Bournemouth, UK. ", 0],
              ["(Special Issue of CASA 2020)", 2],
            ],
            imgs: [
              {
                src:
                  "RGB-D Salient Object Detection via Deep Fusion of Semantics and Details.png",
              },
            ],
            abstract:
              "In this paper, we address RGB‐D salient object detection task by jointly leveraging semantics and contour details of salient objects. We propose a novel semantics‐and‐details complementary fusion network to adaptively integrate cross‐model and multilevel features. Specifically, we employ two kinds of fusion modules in our model, which are designed for fusing high‐level semantic features and integrating contour detail features of the scene components, respectively. The semantics fusion module aggregates high‐level interdependent semantic relationships by a nonlinear weighted summation of small and medium receptive fields. Meanwhile, the details module integrates multi‐level contour detail features to leverage expressive details of salient objects. We achieve new state‐of‐the‐art salient object detection results on seven RGB‐D datasets, that is, STERE, NJU2000, LFSD, NLPR, SSD, DES, and SIP2019 dataset. Experimental results demonstrate that our method outperforms eleven state‐of‐the‐art salient object detection methods. In this paper, we propose a novel semantics‐and‐details complementary fusion network to adaptively integrate cross‐model and multi‐level features. Specifically, we employ two kinds of fusion modules in our model, which are designed for fusing high‐level semantic features and integrating contour detail features of the scene components respectively. The semantics fusion module aggregates high‐level interdependent semantic relationships by a nonlinear weighted summation of small and medium receptive fields. Meanwhile, the details module integrates multi‐level contour detail features to leverage expressive details of salient objects.",
          },
          {
            authors:
              "Sucheng Ren, Chu Han, Xin Yang, Guoqiang Han, Shengfeng He.",
            title:
              "TENet: Triple Excitation Network for Video Salient Object Detection.",
            tags: [
              ["ECCV 2020", 1],
              [", Glasgow, UK. ", 0],
              ["(Spotlight)", 2],
            ],
            imgs: [
              {
                src:
                  "Triple Excitation Network for Video Salient Object Detection.png",
              },
            ],
            abstract: 'In this paper, we propose a simple yet effective approach, named Triple Excitation Network, to reinforce the training of video salient object detection (VSOD) from three aspects, spatial, temporal, and online excitations. These excitation mechanisms are designed following the spirit of curriculum learning and aim to reduce learning ambiguities at the beginning of training by selectively exciting feature activations using ground truth. Then we gradually reduce the weight of ground truth excitations by a curriculum rate and replace it by a curriculum complementary map for better and faster convergence. In particular, the spatial excitation strengthens feature activations for clear object boundaries, while the temporal excitation imposes motions to emphasize spatio-temporal salient regions. Spatial and temporal excitations can combat the saliency shifting problem and conflict between spatial and temporal features of VSOD. Furthermore, our semi-curriculum learning design enables the first online refinement strategy for VSOD, which allows exciting and boosting saliency responses during testing without re-training. The proposed triple excitations can easily plug in different VSOD methods. Extensive experiments show the effectiveness of all three excitation methods and the proposed method outperforms state-of-the-art image and video salient object detection methods.'
          },
        ],
      };
    },
    methods: {
      textBold(s) {
        return textBold(s);
      },
      goBack() {
        goBack();
      },
    },
  });
</script>

</html>