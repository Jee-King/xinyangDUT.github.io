
<!-- saved from url=(0075)https://www.cs.cityu.edu.hk/~rynson/projects/lowlight/Low-lightImaging.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="Generator" content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:SimSun;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Tahoma;
	panose-1:2 11 6 4 3 5 4 4 2 4;}
@font-face
	{font-family:"\@SimSun";
	panose-1:2 1 6 0 3 1 1 1 1 1;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:10.0pt;
	margin-left:0in;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
	{color:purple;
	text-decoration:underline;}
p.MsoAcetate, li.MsoAcetate, div.MsoAcetate
	{mso-style-link:"Balloon Text Char";
	margin:0in;
	margin-bottom:.0001pt;
	font-size:8.0pt;
	font-family:"Tahoma",sans-serif;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:10.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	margin-bottom:.0001pt;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:.5in;
	margin-bottom:.0001pt;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:10.0pt;
	margin-left:.5in;
	line-height:115%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
span.BalloonTextChar
	{mso-style-name:"Balloon Text Char";
	mso-style-link:"Balloon Text";
	font-family:"Tahoma",sans-serif;}
.MsoChpDefault
	{font-family:"Calibri",sans-serif;}
.MsoPapDefault
	{margin-bottom:10.0pt;
	line-height:115%;}
 /* Page Definitions */
 @page WordSection1
	{size:11.0in 17.0in;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

<style type="text/css">@media print{div[id*="composai"] *{display:none !important}}
</style><style data-styled="" data-styled-version="4.3.2"></style></head>

<body lang="EN-US" link="blue" vlink="purple" data-gr-c-s-loaded="true">

<div class="WordSection1">

<div align="center">

<table class="MsoTableGrid" border="1" cellspacing="0" cellpadding="0" width="100%" style="width:100.0%;border-collapse:collapse;border:none">
 <tbody><tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <p class="MsoNormal" align="center" style="margin-top:6.0pt;margin-right:0in;
  margin-bottom:12.0pt;margin-left:0in;text-align:center;line-height:normal"><b><span style="font-size:18.0pt;font-family:&quot;Times New Roman&quot;,serif">Low-light Image
  Analysis</span></b></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"><span style="font-size:12.0pt;font-family:&quot;Times New Roman&quot;,serif">&nbsp;</span></p>
  <p class="MsoNormal" style="margin-bottom:10.0pt;text-align:justify;text-justify:
  inter-ideograph;line-height:115%"><span style="font-size:12.0pt;line-height:
  115%;font-family:&quot;Times New Roman&quot;,serif">Low-light imaging is often needed
  for various purposes, such as surveillance, photography and autonomous
  driving. In particular for autonomous driving, day-time and night-time each
  roughly contributes to 50% of the time over a year, and it is equally
  important for computer vision techniques developed for day-time scenes to work
  at night-time scenes. Unfortunately, low-light images typically contain very
  dark regions, which may suffer from under-exposure problems (i.e., their
  values are very close to zero), while night-time images may suffer from both
  under-exposure as well as over-exposure problems (i.e., their values may be
  very close to either zero or one). Enhancing these images or processing them
  with existing computer vision algorithms often do not work.</span></p>
  <p class="MsoNormal" style="margin-bottom:10.0pt;text-align:justify;text-justify:
  inter-ideograph;line-height:115%"><span style="font-size:12.0pt;line-height:
  115%;font-family:&quot;Times New Roman&quot;,serif">In this project, we are developing
  techniques to process low-light images. Our research is to address this
  problem from two directions. The first is to consider how to enhance these
  images to improve their visibility. The second is to investigate how to
  improve existing computer vision algorithms for direct analyses of low-light
  images. </span></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" width="100%" style="width:100.0%;border-collapse:collapse;border:none">
   <tbody><tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    0in;margin-left:0in;margin-bottom:.0001pt;line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Night-time Semantic
    Segmentation with a Large Real Dataset</span></b><span style="font-family:
    &quot;Times New Roman&quot;,serif"> </span><span style="font-size:10.0pt;font-family:
    &quot;Times New Roman&quot;,serif">[</span><a href="https://arxiv.org/pdf/2003.06883.pdf"><span style="font-size:10.0pt;
    font-family:&quot;Times New Roman&quot;,serif">paper</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [suppl] [code]
    [dataset] </span></p>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">Xin
    Tan, Yiheng Zhang, Ying Cao, Lizhuang Ma, and Rynson Lau </span></p>
    <p class="MsoNormal" style="margin-bottom:6.0pt;line-height:normal"><b><i><span style="font-family:&quot;Times New Roman&quot;,serif;color:#C00000">arXiv:2003.06883</span></i></b><span style="font-family:&quot;Times New Roman&quot;,serif">, March 2020</span></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <div align="center">
    <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
     <tbody><tr style="height:142.8pt">
      <td width="751" valign="top" style="width:513.45pt;padding:0in 5.4pt 0in 5.4pt;
      height:142.8pt">
      <p class="MsoNormal" align="center" style="margin-bottom:0in;margin-bottom:
      .0001pt;text-align:center;line-height:normal"><img border="0" width="737" height="578" src="./Low-lightImaging_files/image001.jpg" title="" style="outline: red dashed 1px;"></p>
      <p class="MsoNormal" style="margin-top:0in;margin-right:-2.9pt;margin-bottom:
      0in;margin-left:0in;margin-bottom:.0001pt;text-align:justify;text-justify:
      inter-ideograph;line-height:normal"><span style="font-size:9.0pt;
      font-family:&quot;Times New Roman&quot;,serif">Visual comparison of our results
      with those of the state-of-the-art methods. Our advantages are
      highlighted by white boxes. A few drawbacks of the other methods are
      marked by yellow boxes. All the methods are trained on NightCity.</span></p>
      </td>
     </tr>
    </tbody></table>
    </div>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal;text-autospace:none"></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Input-Output:
    </span></b><span style="font-family:&quot;Times New Roman&quot;,serif">Given an input
    night-time image, our network directly produces a semantic segmentation map.</span></p>
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Abstract.</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> Although huge progress has
    been made on semantic segmentation in recent years, most existing works
    assume that the input images are captured in day-time with good lighting
    conditions. In this work, we aim to address the semantic segmentation problem
    of night-time scenes, which has two main challenges: 1) labeled night-time
    data are scarce, and 2) over- and under-exposures may co-occur in the input
    night-time images and are not explicitly modeled in existing semantic
    segmentation pipelines. To tackle the scarcity of night-time data, we
    collect a novel labeled dataset (named NightCity) of 4,297 real nighttime
    images with ground truth pixel-level semantic annotations. To our
    knowledge, NightCity is the largest dataset for night-time semantic
    segmentation. In addition, we also propose an exposure-aware framework to address
    the night-time segmentation problem through augmenting the segmentation
    process with explicitly learned exposure features. Extensive experiments
    show that training on NightCity can significantly improve the performance
    of night-time semantic segmentation and that our exposure-aware model
    outperforms the state-of-the-art segmentation methods, yielding top performances
    on our benchmark dataset.</span></p>
    </td>
   </tr>
  </tbody></table>
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" width="100%" style="width:100.0%;border-collapse:collapse;border:none">
   <tbody><tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    0in;margin-left:0in;margin-bottom:.0001pt;line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Learning to Restore Low-light
    Images via Decomposition-and-Enhancement</span></b><span style="font-family:
    &quot;Times New Roman&quot;,serif"> </span><span style="font-size:10.0pt;font-family:
    &quot;Times New Roman&quot;,serif">[<a href="http://www.cs.cityu.edu.hk/~rynson/papers/cvpr20a.pdf">paper</a>] [<a href="http://www.cs.cityu.edu.hk/~rynson/papers/demos/cvpr20a-supp.pdf">suppl</a>]
    [model] [dataset]</span></p>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">Ke
    Xu, Xin Yang, Baocai Yin, and Rynson Lau </span></p>
    <p class="MsoNormal" style="margin-bottom:6.0pt;line-height:normal"><b><i><span style="font-family:&quot;Times New Roman&quot;,serif;color:#C00000">Proc. IEEE CVPR</span></i></b><span style="font-family:&quot;Times New Roman&quot;,serif">, June 2020</span></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <div align="center">
    <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
     <tbody><tr style="height:142.8pt">
      <td width="754" valign="top" style="width:513.45pt;padding:0in 5.4pt 0in 5.4pt;
      height:142.8pt">
      <p class="MsoNormal" align="center" style="margin-bottom:0in;margin-bottom:
      .0001pt;text-align:center;line-height:normal"><span style="font-size:
      9.0pt;font-family:&quot;Times New Roman&quot;,serif"><img border="0" width="739" height="175" id="Picture 2" src="./Low-lightImaging_files/image002.jpg"></span></p>
      <p class="MsoNormal" style="margin-top:3.0pt;margin-right:-2.9pt;
      margin-bottom:0in;margin-left:0in;margin-bottom:.0001pt;text-align:justify;
      text-justify:inter-ideograph;line-height:normal"><span style="font-size:
      9.0pt;font-family:&quot;Times New Roman&quot;,serif">While existing methods ((c) to
      (j)) generally fail to enhance the input noisy low-light image (a), our
      method produces a sharper and clearer result with objects and details
      recovered (l).</span></p>
      </td>
     </tr>
    </tbody></table>
    </div>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal;text-autospace:none"></p>
    </td>
   </tr>
   <tr>
    <td width="100%" valign="top" style="width:100.0%;padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Input-Output:
    </span></b><span style="font-family:&quot;Times New Roman&quot;,serif">Given an input
    practical low-light image, which often comes with a significant amount of
    noise due to the low signal-to-noise ratio, our network enhances its
    brightness while at the same time suppressing its noise level, to produce
    an enhanced clear image.</span></p>
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal;text-autospace:none"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Abstract.</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> Low-light images typically
    suffer from two problems. First, they have low visibility (i.e., small
    pixel values). Second, noise becomes significant and disrupts the image
    content, due to low signal-to-noise ratio. Most existing lowlight image
    enhancement methods, however, learn from noise-negligible datasets. They
    rely on users having good photographic skills in taking images with low
    noise. Unfortunately, this is not the case for majority of the low-light images.
    While concurrently enhancing a low-light image and removing its noise is
    ill-posed, we observe that noise exhibits different levels of contrast in
    different frequency layers, and it is much easier to detect noise in the
    low-frequency layer than in the high one. Inspired by this observation, we
    propose a frequency-based decomposition-and-enhancement model for low-light
    image enhancement. Based on this model, we present a novel network that
    first learns to recover image objects in the low-frequency layer and then
    enhances high-frequency details based on the recovered image objects. In
    addition, we have prepared a new low-light image dataset with real noise to
    facilitate learning. Finally, we have conducted extensive experiments to show
    that the proposed method outperforms state-of-the-art approaches in
    enhancing practical noisy low-light images.</span></p>
    </td>
   </tr>
  </tbody></table>
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"></p>
  </td>
 </tr>
 <tr>
  <td width="100%" valign="top" style="width:100.0%;border:none;border-bottom:
  solid windowtext 1.5pt;padding:0in 5.4pt 0in 5.4pt">
  <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
   <tbody><tr>
    <td valign="top" style="padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    0in;margin-left:0in;margin-bottom:.0001pt;line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif;color:black">Image Correction
    via Deep Reciprocating HDR Transformation</span></b><span style="font-family:
    &quot;Times New Roman&quot;,serif"> </span><span style="font-size:10.0pt;font-family:
    &quot;Times New Roman&quot;,serif">[</span><a href="http://www.cs.cityu.edu.hk/~rynson/papers/cvpr18a.pdf"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">paper</span></a><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">] [<a href="http://www.cs.cityu.edu.hk/~rynson/papers/demos/cvpr18asupp.pdf">suppl</a>]
    [<a href="https://drive.google.com/open?id=1PqZPgBxGw14tukPvtgnPl7qn0Zu6KhTU">code</a>]
    [<a href="https://drive.google.com/open?id=1lNxf_3LW5b1w5uAvFVLJE5By01OmXRXI">dataset</a>]</span></p>
    <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;
    line-height:normal"><span style="font-size:10.0pt;font-family:&quot;Times New Roman&quot;,serif">Xin
    Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, and Rynson Lau </span></p>
    <p class="MsoNormal" style="margin-bottom:6.0pt;line-height:normal"><b><i><span style="font-family:&quot;Times New Roman&quot;,serif;color:#C00000">Proc. IEEE CVPR</span></i></b><span style="font-family:&quot;Times New Roman&quot;,serif">, June 2018</span></p>
    <div align="center">
    <table class="MsoTableGrid" border="0" cellspacing="0" cellpadding="0" style="border-collapse:collapse;border:none">
     <tbody><tr style="height:142.8pt">
      <td width="766" valign="top" style="width:513.45pt;padding:0in 5.4pt 0in 5.4pt;
      height:142.8pt">
      <p class="MsoNormal" align="center" style="margin-bottom:0in;margin-bottom:
      .0001pt;text-align:center;line-height:normal"><img border="0" width="751" height="228" id="Picture 4" src="./Low-lightImaging_files/image003.jpg" alt="t"></p>
      <p class="MsoNormal" style="margin-top:3.0pt;margin-right:-2.9pt;
      margin-bottom:0in;margin-left:0in;margin-bottom:.0001pt;text-align:justify;
      text-justify:inter-ideograph;line-height:normal"><span style="font-size:
      9.0pt;font-family:&quot;Times New Roman&quot;,serif">Image correction results on an
      underexposed input. Existing LDR methods have the limitation in
      recovering the missing details, as shown in (b)-(f). In comparison, we
      recover the missing LDR details in the HDR domain and preserve them
      through tone mapping, producing a more favorable result as shown in (g).</span></p>
      </td>
     </tr>
    </tbody></table>
    </div>
    <p class="MsoNormal" style="margin-bottom:6.0pt;line-height:normal"></p>
    </td>
   </tr>
   <tr>
    <td valign="top" style="padding:0in 5.4pt 0in 5.4pt">
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:115%"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Input-Output:</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> Given an input low-light image,
    our network produces an enhanced image with lost details recovered.</span></p>
    <p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:
    6.0pt;margin-left:0in;text-align:justify;text-justify:inter-ideograph;
    line-height:normal"><b><span style="font-family:&quot;Times New Roman&quot;,serif">Abstract:</span></b><span style="font-family:&quot;Times New Roman&quot;,serif"> Image correction aims to
    adjust an input image into a visually pleasing one. Existing approaches are
    proposed mainly from the perspective of image pixel manipulation. They are
    not effective to recover the details in the under/over exposed regions. In
    this paper, we revisit the image formation procedure and notice that the
    missing details in these regions exist in the corresponding high dynamic
    range (HDR) data. These details are well perceived by the human eyes but
    diminished in the low dynamic range (LDR) domain because of the tone
    mapping process. Therefore, we formulate the image correction task as an
    HDR transformation process and propose a novel approach called Deep
    Reciprocating HDR Transformation (DRHT). Given an input LDR image, we first
    reconstruct the missing details in the HDR domain. We then perform tone
    mapping on the predicted HDR data to generate the output LDR image with the
    recovered details. To this end, we propose a united framework consisting of
    two CNNs for HDR reconstruction and tone mapping. They are integrated
    end-to-end for joint training and prediction. Experiments on the standard
    benchmarks demonstrate that the proposed method performs favorably against
    state-of-the-art image correction methods.</span></p>
    </td>
   </tr>
  </tbody></table>
  <p class="MsoNormal" style="margin-bottom:0in;margin-bottom:.0001pt;line-height:
  normal"></p>
  </td>
 </tr>
</tbody></table>
</div>
<img src="./Low-lightImaging_files/counter.cgi" width="1" height="1">
<p class="MsoNormal" style="margin-top:6.0pt;margin-right:0in;margin-bottom:0in;
margin-left:0in;margin-bottom:.0001pt"><i><span style="font-size:9.0pt;
line-height:115%;font-family:&quot;Times New Roman&quot;,serif">Last updated in March
2020</span></i></p>

</div>




<div id="composai-root"><div id="composai-app"></div></div><div id="ycce-container" style="font-family: -apple-system,system-ui,BlinkMacSystemFont,&#39;Segoe UI&#39;,Roboto,&#39;Helvetica Neue&#39;,Arial,sans-serif;font-size: 14px;font-weight: 400;line-height: 1.5;color: #292b2c;background-color: #fff;margin: 0;"></div></body><div id="composai-cards" style="position: absolute; top: 0px; left: 0px;"></div></html>
